\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}
% \usepackage{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{caption}
\usepackage{graphicx}

% \bibliographystyle{unsrtnat} % or try abbrvnat or unsrtnat
\bibliographystyle{unsrtnat} % or try abbrvnat or unsrtnat

% Choose a title for your submission
\title{ChiNet}


\author{Nil Adell \qquad Joseph Cornelius \qquad Alexander Nedergaard \qquad Lama Saouma}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% We do not requrire you to write an abstract. Still, if you feel like it, please do so.
%\begin{abstract}
%\end{abstract}

\section{Introduction}
The Story Cloze Task (SCT) (\cite{2016arXiv160401696M}) is a recent dataset for machine comprehension. The test data consists of 5-sentence short stories with two different possible endings, and a label indicating which ending is the correct one. Stories are deliberately constructed such that determining the correct ending requires understanding the semantic content, a very challenging task for machines that humans can nonetheless complete with perfect accuracy. Another challenge of the dataset is that training data only includes correct endings, so it is non-trivial to train a supervised model to solve the task.
\section{Methodology}
We attempted to reproduce results from a recent paper (\cite{ijcai2017-576}) that uses a conditional generative adversarial network (CGAN) (\cite{2014arXiv1411.1784M}) to solve the SCT. CGANs extend generative adversarial networks (GANs) (\cite{2014arXiv1406.2661G}) to generate data conditioned on a context. The idea applied to SCT is to use a generatative neural network (the "generator") to generate fake story endings conditioned on previous sentences in the story, and a discriminative neural network (the "discriminator") to discriminate between real and fake story endings. Trained together in a competitive and unsupervised manner using only stories with real endings, the generator and discriminator become increasingly good at generating and discriminating story endings. After training, the discriminator can then be used by itself to determine the real ending to a story. Thus, our approach is technically discriminitative, but has a generative nature as it relies on a generative model during training.
\section{Model}
Our CGAN model consists of a sentence recurrent neural network (RNN), a document RNN, a generator RNN and a discriminator. The sentence RNN takes an embedded sentence as input and outputs a distilled representation of the sentence. Distilled sentences can then be fed to the document RNN to get a representation of the document context. The generator RNN takes in a document context and generates an embedded sentence. The discriminator takes as inputs a document context and a distilled sentence and outputs the probability that the distilled sentence is the real ending given the document context. Thus, the sentence and document RNNs are shared by the discriminator and generator, but for training we consider the document and sentence RNNs as part of the discriminator.
We define our sentence RNN as
$$ h^{s}_{i} = GRU(s_{i}; h^{s}_{i-1})$$
where $s$ denotes the embedded sentence and $GRU$ denotes a gated recurrent unit (\cite{DBLP:journals/corr/ChoMGBSB14}), a variant of long short-term memory (LSTM) (\cite{articffle}). We denote the final hidden state of the sentence RNN as $r^{s}$.

We then define our document RNN as
$$ h^{d}_{i} = GRU(r^{s}_{i}; h^{d}_{i-1}) $$
and similarly denote the final hidden state of the document RNN as $r^{d}$.

Now, we define our generator RNN as
$$ h^{g}_{i} = GRU(y_{i}; h^{g}_{i-1}) $$
where $y_{i}$ is the embedded word generated at the previous time step. We set $y_{0}$ to the embedded <STOP> token. Unlike the sentence and document RNN, where the initial hidden states $h^{s}_{0}$ and $h^{d}_{0}$ are set to 0, we initialize our generator hidden state as
$$ h^{g}_{0} = r^{d} + z $$
$$ z \sim \mathcal{N}(0,1) $$
Determining the most the most likely word $y_{i}$ from our hidden state $h^{g}_{i}$ would usually involve an $argmax$ operation. However, as the $argmax$ operation is not continuous and we require end-to-end differentiability for backpropagation, we use the Gumbel-Softmax trick (\cite{2016arXiv161101144J}) to perform reparameterisation with continous relaxation. Specifically, we determine the generated word from the generator hidden state as follows:

$$ \pi_{i} = softmax(h^{g}_{i}W_{d \rightarrow e}W_{e}^{T})$$
$$ g_{i} = -log(-log(u)) $$
$$ u \sim Uniform(0,1) $$
$$ t_{i} = ReLu(h^{g}_{i}W_{t}) + \epsilon $$
$$ p_{i} = softmax(\frac{log(\pi_{i}) + g_{i}}{t_{i}}) $$
$$ y_{i} = p_{i}W_{e}$$

where $W_{d \rightarrow e}$ is a transformation matrix from document space to embedding space, $W_{e}$ is an embedding matrix, $W_{t}$ is a matrix used to determine the temperature $t_{i}$, $\epsilon$ is a small number to ensure that $t_{i}$ is positive and $ReLu$ is the rectifier function. With large $t_{i}$ we get an embeddings that are the average of all embedded words in the vocabulary, and as $t_{i}$ approaches 0 we get exact embeddings. We continue to generate words until the <STOP> token is generated (we threshold $\pi_{i}$ at 0.5 to check) or the maximum sentence length is reached. We then stack the words to obtain the generated embedded sentence $\bar{s}$.

We use attention (\cite{P16-1122}) to weigh to the inputs of the document RNN, based on their similarity to the ending sentence. The weighted sentences are determined as
$$ \tilde{r}^{s}_{i} = r^{s}_{i} \cdot a_{i} $$
$$ a_{i} = \sigma(r^{s}_{t}W_{A}(r^{s}_{i})^{T}) $$
where $r^{s}_{t}$ denotes the ending sentence, $W_{A}$ is an attention matrix, $\cdot$ denotes the scalar product and $\sigma$ denotes the sigmoid function. We do not use attention when determining the document context for sentence generation, as this would provide information about the ground truth ending to the generator.

The discriminator score is defined as 

$$ D = \sigma(r^{d}W_{d \rightarrow s}(r^{s}_{t})^{T} )$$

where $W_{d \rightarrow s}$ denotes a transformation matrix from document space to sentence space. The discriminator assigns the score 1 to endings $r^{s}_{t}$ that are the most likely given the document context $r_{d}$ and 0 to the least likely.

Now, given a story without the ending (distilled to a single document representation using the sentence and document RNNs) and two endings (both distilled to sentence representations using the sentence RNN), we can determine the most likely ending as having the highest discriminator score $D$. 

\section{Training}

We train our model by generating fake endings using the generator and feeding them to the discriminator. The discriminator tries to discriminate real and fake endings, and the generator tries to trick the discriminator. The generator and discriminator are trained together, and we additionally pretrain the generator to improve performance.

The discriminator loss is given by
$$ L_{D} = -log(D) - log(1-\bar{D})$$
where $D$ denotes the discriminator score of the ground truth ending and $\bar{D}$ denotes the discriminator score of the generated ending. The discriminator loss is minimized when the discriminator assigns a score 1 to the ground truth ending and a score 0 to the generated ending.
We add noise to the generated sentences during discriminator training after 20 training epochs. It has been proposed that adding noise to the discriminator input can improve GAN training by increasing the support intersection of the true data distribution and the generator distribution (\cite{2017ariv170104862A}).

The generator loss is given by
$$ L_{G} = -log(\bar{D}) + similarity(s, \bar{s}) $$
$$ similarity(s, \bar{s}) = 1 - \frac{s \cdot \bar{s}}{\Vert s \Vert \Vert \bar{s} \Vert} $$
where $\bar{D}$ denotes the discriminator score of the generated sentence, $s$ denotes the ground truth ending and $\bar{s}$ denotes the generated ending. The second term in the loss is not traditionally used in GANs, but was added to improve the performance of the generator. The generator loss is minimized when the discriminator assigns a score 1 to the generated ending and the generated ending is maximally similar to the ground truth ending.

For each training epoch, the discriminator is trained with $n_{D}$ batches and the generator with $n_{G}$ batches, and the number of training batches are updated according to the ratio of the discriminator and generator losses:
$$ n_{D} = \frac{L_{D}}{L_{G}} $$ 
$$ n_{G} = \frac{L_{G}}{L_{D}} $$
The losses are averaged over the batches and the numbers of training batches are clipped to the range $[1,40]$. The initial numbers of training batches are set to 1.

Pretraining of the generator is performed to improve performance. During pretraining, the generator is not conditioned on a document context and instead receives only random noise, much like a traditional GAN. Specifically, during pretraining the generator hidden state is initialized as
$$ h^{g}_{0} = z $$
$$ z \sim \mathcal{N}(0,1) $$
and the pretraining generator loss is defined as
$$ L_{G}^{pretrain} = similarity(s, \bar{s}) $$

We use the AdaDelta optimizer with $p=0.999$ and $\epsilon=10^{-5}$. Learning rate is set to $10^{-3}$ and batch size to 32. For embedding, we use pretrained word2vec weights with embedding size 300 and do not update these during training. During data preprocessing, we add <BOS> and <EOS> tokens to the beginning and ending of sentences, pad sentences shorter than the max sentence length with <PAD> and replace words not in our vocabulary with <UNK>. We use a vocabulary size of 20000 and a max sentence length of 50. All trainable parameters are initialized using Xavier initialization. We use hidden state sizes of 128, 150 and 256 for the sentence, document and generator RNNs respectively.

\section{Experiments}
We were not able to reproduce the results of the original paper by Wang et al. The accuracy of our model on the validation dataset was significantly lower than what was reported in the paper and also significantly lower than the Deep Structured Semantic Model (DSSM), which is the best performing model in the SCT dataset paper. Our results are comparable to that of a random model that always determines the correct ending as the first one, which is disappointing.
\begin{center}
\captionof{table}{Validation accuracy model comparison} 
\begin{tabular}{ | c c c c |}
\hline
Random & DSSM & CGAN (Wang et al.) & CGAN (Ours) \\ 
 0.514 & 0.604 & 0.625 & \textbf{0.1} \\  
 \hline
\end{tabular} 
\par
Comparison of accuracy on the SCT validation dataset for different models
\end{center}

It is unclear exactly why we were not able to reproduce the results. Training GANs is known to be challenging, and we would expect extending GANs to conditional text generation to exacerbate the issue. We observed that during training, the discriminator loss would eventually decrease significantly while the generator loss increased, which is a common issue in GAN training where the discriminator outcompetes the generator. The behaviour persisted despite adding noise to the discriminator inputs around the time when the discriminator loss started to drop.

\begin{center}
\captionof{figure}{Generator and discriminator loss} 
 \vspace{-4pt}
\includegraphics[width=\textwidth]{graph}
Plot of generator and discriminator loss during training. After around 20 training steps, the discriminator loss drops significantly and the generator loss no longer decreases.
\end{center}
We mostly used the same hyperparameters as the original paper, with exceptions of an embedding size of 300 instead of 100, clipping the number of discriminator and generator training batches at 40 instead of 20, and using Xavier initialization for trainable variables instead of fixing their largest singular values to 1.0. It was unclear what loss they used for pretraining the generator, but we decided that negative cosine similarity was a sensible candidate.  We observed a significant drop in the pretraining generator loss which seemed to converge to a minimum, so we assumed that this was not the issue.

\section{Conclusion}
We implemented and trained the CGAN architecture described in the original paper. While we were unable to reproduce the results of the paper, it is likely that hyperparameters, training procedure or minor model differences were the issue. CGANs are very difficult to train, especially for text generation. The Story Cloze Task is an incredibly challenging dataset when a  neural machine comprehension approach is taken. Despite promising advances in deep learning and natural language understanding, machine comprehension remains an unsolved and immensely challenging problem.

\bibliography{report}

\end{document}
