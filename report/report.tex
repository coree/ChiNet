\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Choose a title for your submission
\title{ChiNet}


\author{Nil Adell \qquad Joseph Cornelius \qquad Alexander Nedergaard \qquad Lama Saouma}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% We do not requrire you to write an abstract. Still, if you feel like it, please do so.
%\begin{abstract}
%\end{abstract}

Feel free to add more sections but those listed here are strongly recommended.
\section{Introduction}
You can keep this short. Ideally you introduce the task already in a way that highlights the difficulties  your method will tackle.
\section{Methodology}
Your idea. You can rename this section if you like. Early on in this section -- but not necessarily first -- make clear what category your method falls into: Is it generative? Discriminative? Is there a particular additional data source you want to use?
\section{Model}
The math/architecture of your model. This should formally describe your idea from above. If you really want to, you can merge the two sections. \textbf{Xander is on this}

We define our sentence RNN as
$$ h^{s}_{i} = GRU(s_{i}; h^{s}_{i-1})$$
where $s$ denotes the embedded sentence. We denote the final hidden state of the sentence RNN as $r^{s}$. \\
We then define our document RNN as
$$ h^{d}_{i} = GRU(r^{s}_{i}; h^{d}_{i-1}) $$
and similarly denote the final hidden state of the document RNN as $r^{d}$. \\
Now, we define our generator RNN as
$$ h^{g}_{i} = GRU(y_{i}; h^{g}_{i-1}) $$
where $y_{i}$ is the embedded word generated at the previous time step. We set $y_{0}$ to the embedded stop-word. Unlike the sentence and document RNN, where the initial hidden states $h^{s}_{0}$ and $h^{d}_{0}$ are set to 0, we initialize our generator hidden state as
$$ h^{g}_{0} = r^{d} + z $$
$$ z \sim \mathcal{N}(0,1) $$
We determine the generated word from the generator hidden state using Gumbel-Softmax:

$$ \pi_{i} = softmax(h^{g}_{i}W_{d \rightarrow e}W_{e}^{T})$$
$$ g_{i} = -log(-log(u)) $$
$$ u \sim Uniform(0,1) $$
$$ t_{i} = relu(h^{g}_{i}W_{t}) + \epsilon $$
$$ p_{i} = softmax(\frac{log(\pi_{i}) + g_{i}}{t_{i}}) $$
$$ y_{i} = p_{i}W_{e}$$

where $W_{d \rightarrow e}$ is a transformation matrix from document to embedding space, $W_{e}$ is the embedding matrix, $W_{t}$ is a matrix used to determine the temperature $t_{i}$ and $\epsilon$ is a small number to ensure that $t_{i}$ is positive. We continue to generate words until the stop-word is generated (we use $\pi_{i}$ to check) or the maximum sentence length is reached. We then stack the words to obtain the generated embedded sentence $\bar{s}$.

The discriminator score is defined as 

$$ D = \sigma(r^{d}W_{d \rightarrow s}(r^{s}_{t})^{T} )$$

where $W_{d \rightarrow s}$ denotes a transformation matrix from document space to sentence space and $\sigma$ is the sigmoid function. The discriminator assigns the score 1 to target sentences $r^{s}_{t}$ that it considers the most likely ending given the document context $r_{d}$ and 0 to the least likely. \\
We use attention to assign weights to the inputs of the document RNN, based on their similarity to the target sentence. The weighted sentences are determined as
$$ \tilde{r}^{s}_{i} = r^{s}_{i} \cdot a_{i} $$
$$ a_{i} = r^{s}_{t}W_{A}(r^{s}_{i})^{T} $$
where $r^{s}_{t}$ denotes the target sentence, $W_{A}$ is an attention matrix and $\cdot$ denotes the scalar product. We do not use attention when determining the document context for sentence generation, as this would provide information about the ground truth target sentence to the generator.\\ \\
Now, given a story without the ending (distilled to a single document representation using the sentence and document RNNs) and two endings (both distilled to sentence representations using the sentence RNN), we can determine the most likely ending as having the highest discriminator score $D$. 

\section{Training}
What is your objective? How do you optimize it?
\textbf{Xander is on this} \\

We defined different losses for the generator and discriminator, and a separate loss for pretraining the generator. The discriminator loss is given by
$$ L_{D} = -log(D) - log(1-\bar{D})$$
where $D$ denotes the discriminator score of the ground truth sentence and $\bar{D}$ denotes the discriminator score of the generated sentence. The discriminator loss is minimized when the discriminator assigns a score 1 to the ground truth ending and a score 0 to the generated ending. \\
The generator loss is given by
$$ L_{G} = -log(\bar{D}) - similarity(r^{s}_{t}, r^{s}_{g}) $$
where $\bar{D}$ denotes the discriminator score of the generated sentence, $r^{s}_{t}$ denotes the sentence representation of the target sentence, $r^{s}_{t}$ denotes the target representation of the generated sentence, and $similarity$ denotes the cosine similary. The second term in the loss is not traditionally used in GANs, but was added to improve the performance of the generator. The generator loss is minimized when the generator assigns a score 1 to the generated ending and the generated ending is maximally similar to the target ending. \\
To improve the performance of the generator, pretraining was performed. During pretraining, the generator was not conditioned on a document context and generated sentences based only on random noise, much like a standard GAN. Specifically, the generator hidden state was initialized as
$$ h^{g}_{0} = z $$
$$ z \sim \mathcal{N}(0,1) $$
and the pretrain generator loss is defined as
$$ L_{G}^{pretrain} = -similarity(r^{s}_{t}, r^{s}_{g}) $$
Pretraining was first performed on the generator for a number of epochs, and then the generator and discriminator were trained together. During each epoch of training, the discriminator was trained for $n_{D}$ batches and the generator for $n_{G}$ batches; the number of training batches were then updated according to the ratio of the discriminator and generator losses:
$$ n_{D} = \frac{L_{D}}{L_{G}} $$ 
$$ n_{G} = \frac{L_{G}}{L_{D}} $$
The losses were averaged over the batches and the number of training batches were clipped to the range $[1,20]$. The initial number of training batches were initialized to 1 and a batch size of 32 was used.

\section{Experiments}
This {\bf must} at least include the accuracy of your method on the validation set.
\section{Conclusion}
You can keep this short, too.
\end{document}
