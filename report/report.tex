\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Choose a title for your submission
\title{ChiNet}


\author{Nil Adell \qquad Joseph Cornelius \qquad Alexander Nedergaard \qquad Lama Saouma}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% We do not requrire you to write an abstract. Still, if you feel like it, please do so.
%\begin{abstract}
%\end{abstract}

Feel free to add more sections but those listed here are strongly recommended.
\section{Introduction}
You can keep this short. Ideally you introduce the task already in a way that highlights the difficulties  your method will tackle.
\section{Methodology}
Your idea. You can rename this section if you like. Early on in this section -- but not necessarily first -- make clear what category your method falls into: Is it generative? Discriminative? Is there a particular additional data source you want to use?
\section{Model}
The math/architecture of your model. This should formally describe your idea from above. If you really want to, you can merge the two sections. \textbf{Xander is on this}

We define our sentence RNN as
$$ h^{s}_{i} = GRU(s_{i}; h^{s}_{i-1})$$
where $s$ denotes the embedded sentence. We denote the final hidden state of the sentence RNN as $r^{s}$. \\
We then define our document RNN as
$$ h^{d}_{i} = GRU(r^{s}_{i}; h^{d}_{i-1}) $$
and similarly denote the final hidden state of the document RNN as $r^{d}$. \\
Now, we define our generator RNN as
$$ h^{g}_{i} = GRU(y_{i}; h^{g}_{i-1}) $$
where $y_{i}$ is the embedded word generated at the previous time step. We set $y_{0}$ to the embedded stop-word. Unlike the sentence and document RNN, where the initial hidden states $h^{s}_{0}$ and $h^{d}_{0}$ are set to 0, we initialize our generator hidden state as
$$ h^{g}_{0} = r^{d} + z $$
$$ z \sim \mathcal{N}(0,1) $$
We determine the generated word from the generator hidden state using Gumbel-Softmax:

$$ \pi_{i} = softmax(h^{g}_{i}W_{d \rightarrow e}W_{e}^{T})$$
$$ g_{i} = -log(-log(u)) $$
$$ u \sim Uniform(0,1) $$
$$ t_{i} = relu(h^{g}_{i}W_{t}) + \epsilon $$
$$ p_{i} = softmax(\frac{log(\pi_{i}) + g_{i}}{t_{i}}) $$
$$ y_{i} = p_{i}W_{e}$$

where $W_{d \rightarrow e}$ is a transformation matrix from document to embedding space, $W_{e}$ is the embedding matrix, $W_{t}$ is a matrix used to determine the temperature $t_{i}$ and $\epsilon$ is a small number to ensure that $t_{i}$ is positive. We continue to generate words until the stop-word is generated (we use $\pi_{i}$ to check) or the maximum sentence length is reached. We then stack the words to obtain the generated embedded sentence $\bar{s}$.

The discriminator score is defined as 

$$ D = \sigma(r^{d}W_{d \rightarrow s}(r^{s}_{t})^{T} )$$

where $W_{d \rightarrow s}$ denotes a transformation matrix from document space to sentence space and $\sigma$ is the sigmoid function. The discriminator assigns the score 1 to target sentences $r^{s}_{t}$ that it considers the most likely ending given the document context $r_{d}$ and 0 to the least likely. \\
We use attention to assign weights to the inputs of the document RNN, based on their similarity to the target sentence. The weighted sentences are determined as
$$ \tilde{r}^{s}_{i} = r^{s}_{i} \cdot a_{i} $$
$$ a_{i} = r^{s}_{t}W_{A}(r^{s}_{i})^{T} $$
where $r^{s}_{t}$ denotes the target sentence, $W_{A}$ is an attention matrix and $\cdot$ denotes the scalar product. We do not use attention when determining the document context for sentence generation, as this would provide information about the ground truth target sentence to the generator.\\ \\
Now, given a story without the ending (distilled to a single document representation using the sentence and document RNNs) and two endings (both distilled to sentence representations using the sentence RNN), we can determine the most likely ending as having the highest discriminator score $D$. 

\section{Training}
What is your objective? How do you optimize it?
\textbf{Xander is on this} \\

The discriminator loss is given by
$$ L_{D} = -log(D) - log(1-\bar{D})$$
where $D$ denotes the discriminator score of the ground truth sentence and $\bar{D}$ denotes the discriminator score of the generated sentence. The discriminator loss is minimized when the discriminator assigns a score 1 to the ground truth ending and a score 0 to the generated ending. \\
The generator loss is given by
$$ L_{G} = -log(\bar{D}) + similarity(r^{s}_{t}, r^{s}_{g}) $$
$$ similarity(r^{s}_{t}, r^{s}_{g}) = 1 - \frac{r^{s}_{t} \cdot r^{s}_{g}}{\Vert r^{s}_{t} \Vert \Vert r^{s}_{g} \Vert} $$
where $\bar{D}$ denotes the discriminator score of the generated sentence, $r^{s}_{t}$ denotes the sentence representation of the target sentence and $r^{s}_{t}$ denotes the target representation of the generated sentence. The second term in the loss is not traditionally used in GANs, but was added to improve the performance of the generator. The generator loss is minimized when the generator assigns a score 1 to the generated ending and the generated ending is maximally similar to the target ending. \\ \\
During each epoch of training, the discriminator was trained for $n_{D}$ batches and the generator for $n_{G}$ batches, and the number of training batches were updated according to the ratio of the discriminator and generator losses:
$$ n_{D} = \frac{L_{D}}{L_{G}} $$ 
$$ n_{G} = \frac{L_{G}}{L_{D}} $$
The losses were averaged over the batches and the number of training batches were clipped to the range $[1,20]$. The initial number of training batches were set to 1.
We added noise to the generated sentences during discriminator training after 20**** training epochs. It has been proposed that adding noise to the discriminator input can theoretically improve GAN training by increasing the support intersection of the generator and true data distributions.

To improve the performance of the generator, pretraining was performed. During pretraining, the generator was not conditioned on a document context, much like a standard GAN. Specifically, the generator hidden state was initialized as
$$ h^{g}_{0} = z $$
$$ z \sim \mathcal{N}(0,1) $$
and the pretrain generator loss is defined as
$$ L_{G}^{pretrain} = similarity(r^{s}_{t}, r^{s}_{g}) $$

We used the AdaDelta optimizer with $p=0.999$ and $\epsilon=1e-5$, a learning rate of $1e-3$ and batch size 32. For embedding, we used pretrained word2vec weights (link***) and did not update these during training. During data preprocessing, we added <BOS> and <EOS> tokens beginning and ends of sentences, padded sentences shorter than the max sentence length with <PAD> and replaced words not in our vocabulary with <UNK>. We used a vocabulary size of 25000**** and a max sentence length of 50. All trainable parameters were initialized using Xavier initialization. We used hidden state sizes of 128, 150 and 256 for the sentence, document and generator RNNs respectively.

\section{Experiments}
This {\bf must} at least include the accuracy of your method on the validation set.
\section{Conclusion}
You can keep this short, too.
\end{document}
